{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange,repeat\n",
    "from typing import Union, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "\ttorch.set_default_dtype(torch.float16)\n",
    "\ttorch.set_default_device(\"cuda\")\n",
    "\tDTYPE = torch.float16\n",
    "else:\n",
    "\ttorch.set_default_dtype(torch.float32)\n",
    "\tDTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# ----------------------\n",
    "\n",
    "NUM_EXCITATORY = 8000\n",
    "NUM_INHIBITORY = 2000\n",
    "P_CONNECTION = 0.20\n",
    "\n",
    "# for generalizability, all connections could be slow or fast\n",
    "RATIO_SLOW_EXCITATORY_EXCITATORY = 0.5\n",
    "RATIO_SLOW_INHIBITORY_INHIBITORY = 0.1\n",
    "RATIO_SLOW_EXCITATORY_INHIBITORY = 0\n",
    "RATIO_SLOW_INHIBITORY_EXCITATORY = 0\n",
    "\n",
    "P_POTENTIATED = 0.2 # 0.2\n",
    "J_POTENTIATED = 0.21 # 0.21\n",
    "J_DEPRESSED = 0.03 # 0.03\n",
    "J_E_TO_I = 0.08 # 0.08\n",
    "J_I_TO_E = -0.18 #-0.18\n",
    "J_I_TO_I = -0.18 #-0.18\n",
    "\n",
    "# ----------------------\n",
    "# ----------------------\n",
    "\n",
    "REFRACTORY_TIME = 2\n",
    "MEMBRANE_REFRACTORY_VEC = REFRACTORY_TIME*torch.ones([NUM_EXCITATORY + NUM_INHIBITORY])\n",
    "\n",
    "MEMBRANE_CONST_EXCITATORY = 20\n",
    "MEMBRANE_CONST_INHIBITORY = 10\n",
    "MEMBRANE_CONST_VEC = torch.concat((MEMBRANE_CONST_EXCITATORY*torch.ones(NUM_EXCITATORY),\n",
    "\t\t\t\t\t\t  MEMBRANE_CONST_INHIBITORY*torch.ones(NUM_INHIBITORY)))\n",
    "\n",
    "THRESHOLD_POTENTIAL = 20\n",
    "V_RESET_EXCITATORY = 15\n",
    "V_RESET_INHIBITORY = 10\n",
    "THRESHOLD_POTENTIAL_VECTOR = THRESHOLD_POTENTIAL*torch.ones([NUM_EXCITATORY + NUM_INHIBITORY])\n",
    "V_RESET_VECT = torch.concat((V_RESET_EXCITATORY*torch.ones(NUM_EXCITATORY),\n",
    "\t\t\t\t\t\t\t V_RESET_INHIBITORY*torch.ones(NUM_INHIBITORY)))\n",
    "\n",
    "# ----------------------\n",
    "# ----------------------\n",
    "\n",
    "DT = 0.1\n",
    "T_MAX = 1000\n",
    "TIMELINE = torch.linspace(0,T_MAX,int(T_MAX/DT))\n",
    "T_IDX_MAX = len(TIMELINE)\n",
    "\n",
    "# ----------------------\n",
    "# ----------------------\n",
    "\n",
    "TAU_S_EXCITATORY = \t10\n",
    "TAU_S_INHIBITORY = 10\n",
    "TAU_S_VEC = torch.concat((TAU_S_EXCITATORY*torch.ones(NUM_EXCITATORY),\n",
    "\t\t\t\t\t\t  TAU_S_INHIBITORY*torch.ones(NUM_INHIBITORY)))\n",
    "\n",
    "# ----------------------\n",
    "# ----------------------\n",
    "\n",
    "TAU_RECOVERY = 200\n",
    "\n",
    "# ----------------------\n",
    "# ----------------------\n",
    "\n",
    "ALPHA_PLASTICITY_RECOVERY = 0.0147\n",
    "BETA_PLASTICITY_RECOVERY = 0.01\n",
    "THETA_X_PLASTICITY_THRESHOLD = 0.4\n",
    "U_SPIKING_COST = 0.45\n",
    "\n",
    "# ----------------------\n",
    "# ----------------------\n",
    "\n",
    "A_HEBBIAN = 0.25\n",
    "B_HEBBIAN = 0.17\n",
    "THETA_LTP = 17.5\n",
    "THETA_LTD = 15.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We first initialize the neurons and the connectivity between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sparse(m:int,\n",
    "\t\t\t\t  n:int,\n",
    "\t\t\t\t  p:float)->torch.Tensor:\n",
    "\t\"\"\"\n",
    "\tFunction for creating a sparse m x n with zeros and ones with some probability.\n",
    "\n",
    "\tArgs:\n",
    "\t\tm: number of post-synaptic neurons\n",
    "\t\tn: number of pre-synaptic neurons\n",
    "\t\tp: probability of connection\n",
    "\t\n",
    "\tReturns:\n",
    "\t\tmat: m x n sparse matrix\n",
    "\t\"\"\"\n",
    "\tmat = torch.zeros(m,n)\n",
    "\tmat[torch.rand(m,n)<p] = 1\n",
    "\treturn mat\n",
    "\n",
    "def sparse_square(n,p):\n",
    "\t\"\"\"\n",
    "\tFunction for creating a n x n sparse matrix with zeros on the diagonal\n",
    "\n",
    "\tArgs:\n",
    "\t\tn: number of neurons\n",
    "\t\tp: probability of connection\n",
    "\n",
    "\tReturns:\n",
    "\t\tmat: n x n sparse matrix\n",
    "\t\"\"\"\n",
    "\tmat = torch.zeros(n,n)\n",
    "\tmat[torch.rand(n,n)<p] = 1\n",
    "\treturn mat*(1-torch.eye(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_fast_pair(m:int,\n",
    "\t\t\t\t   n:int,\n",
    "\t\t\t\t   p_connection:float,\n",
    "\t\t\t\t   ratio_slow:float,)->torch.Tensor:\n",
    "\n",
    "\t\"\"\"\n",
    "\tFunction for obtaining pairs of connectivity matrices for slow/fast currents.\n",
    "\tThe matrices will not overlap so connections can either be slow or fast.\n",
    "\n",
    "\tArgs:\n",
    "\t\tm: number of post-synaptic neurons\n",
    "\t\tn: number of pre-synaptic neurons\n",
    "\t\tp_connection: probability of connection:\n",
    "\t\tratio_slow: ratio of slow connection over all connections\n",
    "\n",
    "\tReturns:\n",
    "\t\tslow: sparse matrix for slow currents\n",
    "\t\tfast: sparse matrix for fast currents\n",
    "\t\"\"\"\n",
    "\n",
    "\tp_slow = p_connection*ratio_slow\n",
    "\tp_fast = p_connection*(1-ratio_slow)\n",
    "\n",
    "\tif m==n:\n",
    "\t\tslow = sparse_square(m,p_slow)\n",
    "\t\tfast = sparse_square(m,p_fast)\n",
    "\t\tfast = (1-slow)*fast\n",
    "\t\treturn slow,fast\n",
    "\n",
    "\telse:\n",
    "\t\tslow = random_sparse(m,n,p_slow)\n",
    "\t\tfast = random_sparse(m,n,p_fast)\n",
    "\t\tfast = (1-slow)*fast\n",
    "\t\treturn slow,fast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_j(u_mat:torch.Tensor,\n",
    "\t\t\t\t p_pot:float,\n",
    "\t\t\t\t j_p:float,\n",
    "\t\t\t\t j_d:float)->Tuple[torch.Tensor]:\n",
    "\n",
    "\t\"\"\"\n",
    "\tFunction for filling connectivity matrix with one of two possible conductance value.\n",
    "\tAlso initializes the hidden synaptic variable L\n",
    "\n",
    "\tArgs:\n",
    "\t\tu_mat: connectivity matrix\n",
    "\t\tp_pot: probability for potentiated conductance\n",
    "\t\tj_p: conductivity for potentiated synapse\n",
    "\t\tj_d: conductivity for depressed synapse\n",
    "\n",
    "\tReturns:\n",
    "\t\tconductance matrix\n",
    "\t\"\"\"\n",
    "\tj = j_d*torch.ones_like(u_mat)\n",
    "\tinitalization = torch.rand_like(j)\n",
    "\tx_init = torch.zeros_like(u_mat)\n",
    "\tx_init[initalization<=p_pot] = 1\n",
    "\tj[initalization<=p_pot] = j_p\n",
    "\treturn (j*u_mat,x_init*u_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connectivity_matrices():\n",
    "\n",
    "\tU_e_to_e_slow,U_e_to_e_fast = slow_fast_pair(NUM_EXCITATORY,NUM_EXCITATORY,P_CONNECTION,RATIO_SLOW_EXCITATORY_EXCITATORY)\n",
    "\tU_i_to_e_slow,U_i_to_e_fast = slow_fast_pair(NUM_EXCITATORY,NUM_INHIBITORY,P_CONNECTION,RATIO_SLOW_EXCITATORY_INHIBITORY)\n",
    "\tU_e_to_i_slow,U_e_to_i_fast = slow_fast_pair(NUM_INHIBITORY,NUM_EXCITATORY,P_CONNECTION,RATIO_SLOW_INHIBITORY_EXCITATORY)\n",
    "\tU_i_to_i_slow,U_i_to_i_fast = slow_fast_pair(NUM_INHIBITORY,NUM_INHIBITORY,P_CONNECTION,RATIO_SLOW_INHIBITORY_INHIBITORY)\n",
    "\n",
    "\tU_to_e_fast = torch.concat((U_e_to_e_fast,U_i_to_e_fast),1)\n",
    "\tU_to_i_fast = torch.concat((U_e_to_i_fast,U_i_to_i_fast),1)\n",
    "\tU_fast = torch.concat((U_to_e_fast,U_to_i_fast))\n",
    "\n",
    "\tU_to_e_slow = torch.concat((U_e_to_e_slow,U_i_to_e_slow),1)\n",
    "\tU_to_i_slow = torch.concat((U_e_to_i_slow,U_i_to_i_slow),1)\n",
    "\tU_slow = torch.concat((U_to_e_slow,U_to_i_slow))\n",
    "\n",
    "\tJ_e_to_e_fast,l_e_to_e_fast = initialize_j(U_e_to_e_fast,p_pot=P_POTENTIATED,j_p=J_POTENTIATED,j_d=J_DEPRESSED)\n",
    "\tJ_e_to_e_slow,l_e_to_e_slow = initialize_j(U_e_to_e_slow,p_pot=P_POTENTIATED,j_p=J_POTENTIATED,j_d=J_DEPRESSED)\n",
    "\t\n",
    "\tJ_i_to_e_slow,_ = initialize_j(U_i_to_e_slow,p_pot=1,j_p=J_I_TO_E,j_d=0)\n",
    "\tJ_i_to_e_fast,_ = initialize_j(U_i_to_e_fast,p_pot=1,j_p=J_I_TO_E,j_d=0)\n",
    "\n",
    "\tJ_e_to_i_slow,_ = initialize_j(U_e_to_i_slow,p_pot=1,j_p=J_E_TO_I,j_d=0)\n",
    "\tJ_e_to_i_fast,_ = initialize_j(U_e_to_i_fast,p_pot=1,j_p=J_E_TO_I,j_d=0)\n",
    "\n",
    "\tJ_i_to_i_slow,_ = initialize_j(U_i_to_i_slow,p_pot=1,j_p=J_I_TO_I,j_d=0)\n",
    "\tJ_i_to_i_fast,_ = initialize_j(U_i_to_i_fast,p_pot=1,j_p=J_I_TO_I,j_d=0)\n",
    "\t\n",
    "\tJ_to_e_fast = torch.concat((J_e_to_e_fast,J_i_to_e_fast),1)\n",
    "\tJ_to_i_fast = torch.concat((J_e_to_i_fast,J_i_to_i_fast),1)\n",
    "\tJ_fast = torch.concat((J_to_e_fast,J_to_i_fast))\n",
    "\n",
    "\tJ_to_e_slow = torch.concat((J_e_to_e_slow,J_i_to_e_slow),1)\n",
    "\tJ_to_i_slow = torch.concat((J_e_to_i_slow,J_i_to_i_slow),1)\n",
    "\tJ_slow = torch.concat((J_to_e_slow,J_to_i_slow))\n",
    "\n",
    "\tw_excitatory = torch.concat((U_e_to_e_fast + U_e_to_e_slow, torch.zeros_like(U_i_to_e_fast)),1)\n",
    "\tw_inhibitory = torch.zeros_like((U_to_i_fast))\n",
    "\tw_mat = torch.concat((w_excitatory,w_inhibitory))\n",
    "\n",
    "\tl_excitatory = torch.concat((l_e_to_e_fast+l_e_to_e_slow,torch.zeros_like(U_i_to_e_fast)),1)\n",
    "\tl_mat = torch.concat((l_excitatory,w_inhibitory))\n",
    "\n",
    "\treturn U_slow,U_fast,J_slow,J_fast,w_mat,l_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(v_t:torch.Tensor,\n",
    "\t\t  v_th:torch.Tensor)->torch.Tensor:\n",
    "\t\"\"\"\n",
    "\tFunction for verifying which neurons are spiking\n",
    "\n",
    "\tArgs:\n",
    "\t\tv_t: membrane potentials at time t\n",
    "\t\tv_th: vector of threshold potentials (may not be uniform)\n",
    "\n",
    "\tReturns:\n",
    "\t\ttensor of 0 (not spiking), 1 (spiking) \n",
    "\t\"\"\"\n",
    "\treturn (v_t>=v_th).to(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_dt(i_s:torch.Tensor,\n",
    "\t\t   tau_s:float,\n",
    "\t\t   dt:float,\n",
    "\t\t   j_slow:torch.Tensor,\n",
    "\t\t   x_sigma_v:torch.Tensor)->torch.Tensor:\n",
    "\n",
    "\t\"\"\"\n",
    "\tFunction for obtaining the update using forward Euler for slow current dynamics\n",
    "\ttau_s di/dt = - i_s + j sigma(v,v_th)\n",
    "\n",
    "\tArgs:\n",
    "\t\ti_s: slow currents at time t\n",
    "\t\ttau_s: time constant for slow currents\n",
    "\t\tdt: time step for forward Euler\n",
    "\t\tu_slow: synaptic conductance matrix for slow currents\n",
    "\t\tx_sigma_v: boolean vector for spiking neurons\n",
    "\n",
    "\tReturns:\n",
    "\t\tupdate for slow currents at time t+1\n",
    "\t\"\"\"\n",
    "\treturn (dt/tau_s)*(-i_s + j_slow @ x_sigma_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short term plasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dx_dt(x:torch.Tensor,\n",
    "\t\t  u:float,\n",
    "\t\t  tau_r:float,\n",
    "\t\t  x_sigma_v:torch.Tensor,\n",
    "\t\t  dt:float)->torch.Tensor:\n",
    "\n",
    "\t\"\"\"\n",
    "\tFunction for obtaining the update using forward Euler for synaptic resource dynamics\n",
    "\n",
    "\tArgs:\n",
    "\t\tx: resources at time t\n",
    "\t\tu: cost of spiking\n",
    "\t\ttau_r: recovery time\n",
    "\t\tx_sigma_v: boolean vector for spiking neurons\n",
    "\t\tdt: time step for forward Euler\n",
    "\n",
    "\tReturns:\n",
    "\t\tupdate for slow currents at time t+1\n",
    "\t\"\"\"\n",
    "\treturn dt*((1-x)/tau_r)-u*x_sigma_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-term plasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_t(l:torch.Tensor,\n",
    "\t\talpha:float,\n",
    "\t\tbeta:float,\n",
    "\t\ttheta_x:float)->torch.Tensor:\n",
    "\n",
    "\t\"\"\"\n",
    "\tFunction for computing the recovery term for long-term plasticity dynamics\n",
    "\n",
    "\tArgs:\n",
    "\t\tl: plasticity latent variable vector at time t\n",
    "\t\talpha: upward recovery factor\n",
    "\t\tbeta: downward recovery factor\n",
    "\t\ttheta_x: plasticity threshold\n",
    "\n",
    "\tReturns:\n",
    "\t\tvector of recovery term at time t\n",
    "\t\"\"\"\n",
    "\treturn -alpha*((theta_x-l)>=0) + beta*((l-theta_x)>=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_v(v:torch.Tensor,\n",
    "\t\ta:float,\n",
    "\t\tb:float,\n",
    "\t\ttheta_ltp:float,\n",
    "\t\ttheta_ltd:float,\n",
    "\t\tv_th:torch.Tensor,\n",
    "\t\tv_reset:torch.Tensor)->torch.Tensor:\n",
    "\t\"\"\"\n",
    "\tFunction for verifying if post and pre-synaptic neurons are firing together.\n",
    "\n",
    "\tArgs:\n",
    "\t\tv: membrane potential at time t\n",
    "\t\ta: step up value for synchronous firing\n",
    "\t\tb: step down value for asynchronous firing\n",
    "\t\ttheta_ltp: threshold potential for long-term potentiation\n",
    "\t\ttheta_ltd: threshold potential for long-term depression\n",
    "\t\tv_th: threshold potential for action potential firing\n",
    "\t\tv_reset: reset potential\n",
    "\n",
    "\tReturns:\n",
    "\t\tvector for tracking synchronous firing\n",
    "\t\"\"\"\n",
    "\treturn a*(theta_ltp<=v)*(v<=v_th) - b*(v<=theta_ltd)*(v<=v_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_dt(h_t:torch.Tensor,\n",
    "\t\t  w:torch.Tensor,\n",
    "\t\t  r_t:torch.Tensor,\n",
    "\t\t  dt:float):\n",
    "\t\n",
    "\t\"\"\"\n",
    "\tFunction for the long-term plasticity latent variable update with forward Euler\n",
    "\n",
    "\tArgs:\n",
    "\t\th_t: Hebbian term matrix\n",
    "\t\tw: masking matrix for excitatory-excitatory synapses\n",
    "\t\tr_t: recovery term matrix\n",
    "\t\tdt: time step for forward Euler\n",
    "\n",
    "\tReturns:\n",
    "\t\tn x n latent variable for plasticity\n",
    "\t\"\"\"\n",
    "\treturn dt*w*(r_t+h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_j(j:torch.Tensor,\n",
    "\t\t\t j_p:float,\n",
    "\t\t\t j_d:float,\n",
    "\t\t\t threshold:float,\n",
    "\t\t\t l_old:torch.Tensor,\n",
    "\t\t\t l_new:torch.Tensor):\n",
    "\n",
    "\t\"\"\"\n",
    "\tFunction for updating conductivity matrices based on plasticity latent variable\n",
    "\n",
    "\tArgs:\n",
    "\t\tj: conductivity matrix\n",
    "\t\tj_p: potentiated conductance\n",
    "\t\tj_d: depressed conductance\n",
    "\t\tthreshold: latent variable threshold for long-term plasticity\n",
    "\t\tl_old: latent variable value at time t\n",
    "\t\tl_new: updated latent variable value\n",
    "\n",
    "\tReturns:\n",
    "\t\tupdated conductivity matrix\n",
    "\t\"\"\"\n",
    "\ta = (l_old<=threshold)*(threshold<l_new)\n",
    "\tb = (l_old>=threshold)*(threshold>l_new)\n",
    "\treturn j_p*a + j_d*b + j*(1-(a+b).int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membrane dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def da_dt(v:torch.Tensor,\n",
    "\t\t  tau_r:torch.Tensor,\n",
    "\t\t  v_th:torch.Tensor,\n",
    "\t\t  dt:float):\n",
    "\t\n",
    "\t\"\"\"\n",
    "\tFunction for the spiking hidden variable update using forward Euler\n",
    "\n",
    "\tArgs:\n",
    "\t\tv: membrane potential at time t\n",
    "\t\ttau_r: vector of membrane refractory time constant\n",
    "\t\tv_th: vector of membrane threshold potentials\n",
    "\t\tdt: time step for forward Euler\n",
    "\n",
    "\tReturns:\n",
    "\t\tvector of spiking hidden variable  update\n",
    "\t\"\"\"\n",
    "\treturn (1/tau_r)*dt*(v>=v_th)\n",
    "\n",
    "def dv_dt(v:torch.Tensor,\n",
    "\t\t  i:torch.Tensor,\n",
    "\t\t  tau_m:torch.Tensor,\n",
    "\t\t  v_th:torch.Tensor,\n",
    "\t\t  dt:float):\n",
    "\n",
    "\t\"\"\"\n",
    "\tFunction for the membrane potential update using forward Euler\n",
    "\n",
    "\tArgs:\n",
    "\t\tv: membrane potential at time t\n",
    "\t\ti: total currents at time t\n",
    "\t\ttau_m: vector of membrane \n",
    "\t\tv_th: vector of membrane threshold potentials\n",
    "\t\tdt: time step for forward Euler\n",
    "\n",
    "\tReturns:\n",
    "\t\tvector of spiking hidden variable update\n",
    "\t\"\"\"\n",
    "\treturn (-v/tau_m+i)*dt*(v<=v_th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_slow,U_fast,J_slow,J_fast,W_mat,L_mat = get_connectivity_matrices()\n",
    "NUM_NEURONS = NUM_EXCITATORY + NUM_INHIBITORY\n",
    "V_MEMBRANE_POTENTIALS = torch.zeros([NUM_EXCITATORY + NUM_INHIBITORY,T_IDX_MAX])\n",
    "A_SPIKING_STATE_VAR = torch.zeros_like(V_MEMBRANE_POTENTIALS)\n",
    "X_RESOURCE_STATE_VAR = torch.ones_like(V_MEMBRANE_POTENTIALS)\n",
    "I_S_SLOW_CURRENTS = torch.zeros_like(V_MEMBRANE_POTENTIALS)\n",
    "I_F_FAST_CURRENTS = torch.zeros_like(V_MEMBRANE_POTENTIALS)\n",
    "\n",
    "AMPLITUDES = 5*torch.rand([NUM_NEURONS])\n",
    "AMPLITUDES = rearrange(AMPLITUDES,\"n -> n 1\")\n",
    "PHASES = 10*torch.rand([NUM_NEURONS])\n",
    "FREQUENCIES = 5*torch.rand([NUM_NEURONS])+5\n",
    "\n",
    "t_sin = repeat(TIMELINE,\"t ->t n\",n=NUM_NEURONS)\n",
    "t_sin = (t_sin/FREQUENCIES) + PHASES\n",
    "t_sin = rearrange(t_sin,\"t n -> n t\")\n",
    "\n",
    "I_EXT = AMPLITUDES*torch.sin(t_sin)+AMPLITUDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(I_EXT[0].cpu())\n",
    "plt.plot(I_EXT[1].cpu())\n",
    "plt.plot(I_EXT[2].cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stored variables:\n",
    "  - $V(t)$: membrane potential\n",
    "  - $I_s(t)$: slow currents\n",
    "  - $I_f(t)$: fast currents\n",
    "  - $I_t(t)$: total currents \n",
    "  - $I_e(t)$: external currents\n",
    "  - $x(t)$: synaptic resources\n",
    "  - $a(t)$: for keeping track of spiking\n",
    "- Passed states (not saved):\n",
    "  - $L$: square matrix\n",
    "  - $R$: square matrix (fully described by $L$ if necessary)\n",
    "  - $H$: square matrix fully described by $v,a$\n",
    "  - $J$: Not sure, it's kind of expensive to track since it's a square matrix but studying learning dynamics could be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_step(v:torch.Tensor,\n",
    "\t\t\t\t\ti_s:torch.Tensor,\n",
    "\t\t\t\t\ti_f:torch.Tensor,\n",
    "\t\t\t\t\ti_e:torch.Tensor,\n",
    "\t\t\t\t\tx:torch.Tensor,\n",
    "\t\t\t\t\ta:torch.Tensor,\n",
    "\t\t\t\t\tl:torch.Tensor,\n",
    "\t\t\t\t\tj_slow:torch.Tensor,\n",
    "\t\t\t\t\tj_fast:torch.Tensor,\n",
    "\t\t\t\t\tw:torch.Tensor,\n",
    "\t\t\t\t\tu_slow:torch.Tensor,\n",
    "\t\t\t\t\tu_fast:torch.Tensor):\n",
    "\t\n",
    "\t# we must first compute the total currents and update the membrane potential\n",
    "\n",
    "\tsigma_v = sigma(v,THRESHOLD_POTENTIAL_VECTOR)\n",
    "\tx_sigma_v = x*sigma_v\n",
    "\n",
    "\ti_s_new = i_s + dis_dt(i_s,TAU_S_VEC,DT,j_slow,x_sigma_v)\n",
    "\n",
    "\ti_f_new = j_fast @ x_sigma_v\n",
    "\t# i_f_new = torch.sparse.mm(j_fast.to(torch.float32).to_sparse(),x_sigma_v.to(torch.float32).unsqueeze(-1))\n",
    "\t# i_f_new = i_f_new.to(DTYPE).squeeze()\n",
    "\n",
    "\ti_t = i_s + i_f + i_e\n",
    "\n",
    "\t# we can then update the membrane potential\n",
    "\n",
    "\tda = da_dt(v,MEMBRANE_REFRACTORY_VEC,THRESHOLD_POTENTIAL_VECTOR,DT)\n",
    "\tdv = dv_dt(v,i_t,MEMBRANE_CONST_VEC,THRESHOLD_POTENTIAL_VECTOR,DT)\n",
    "\ta_new = torch.zeros_like(a) + (a+da)*(a<1)\n",
    "\tv_new = V_RESET_VECT*(a>=1) + (v+dv)*(a<1)\n",
    "\n",
    "\t# finally, we can update the hidden variables for plasticity\n",
    "\n",
    "\tx_new = x + dx_dt(x,U_SPIKING_COST,TAU_RECOVERY,x_sigma_v,DT)\n",
    "\n",
    "\tr_mat = r_t(l=l,alpha=ALPHA_PLASTICITY_RECOVERY,beta=BETA_PLASTICITY_RECOVERY,\n",
    "\t\t\t theta_x=THETA_X_PLASTICITY_THRESHOLD)\n",
    "\n",
    "\tf_v_t = f_v(v,A_HEBBIAN,B_HEBBIAN,THETA_LTP,THETA_LTD,THRESHOLD_POTENTIAL_VECTOR,\n",
    "\t\t\t V_RESET_VECT)\n",
    "\t\n",
    "\th_t = rearrange(f_v_t,\"n -> n 1\") @ rearrange(sigma_v,\"n -> 1 n\")\n",
    "\t\n",
    "\tl_new = l + dl_dt(h_t,w,r_mat,DT)\n",
    "\n",
    "\t# we don't want to create any new slow/fast connection so we need to mask\n",
    "\t\n",
    "\tj_fast_new = u_fast*update_j(j_fast,J_POTENTIATED,J_DEPRESSED,THETA_X_PLASTICITY_THRESHOLD,\n",
    "\t\t\t\t\t   l,l_new)\n",
    "\t\n",
    "\tj_slow_new = u_slow*update_j(j_slow,J_POTENTIATED,J_DEPRESSED,THETA_X_PLASTICITY_THRESHOLD,\n",
    "\t\t\t\t\t   l,l_new)\n",
    "\t\n",
    "\treturn v_new,i_s_new,i_f_new,x_new,a_new,j_slow_new,j_fast_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx in tqdm(range(T_IDX_MAX-1)):\n",
    "\tv_t = V_MEMBRANE_POTENTIALS[:,idx]\n",
    "\ti_s_t = I_S_SLOW_CURRENTS[:,idx]\n",
    "\ti_f_t = I_F_FAST_CURRENTS[:,idx]\n",
    "\ti_e_t = I_EXT[:,idx]\n",
    "\tx_t = X_RESOURCE_STATE_VAR[:,idx]\n",
    "\ta_t = A_SPIKING_STATE_VAR[:,idx]\n",
    "\n",
    "\tv_new,i_s_new,i_f_new,x_new,a_new,J_slow,J_fast = simulation_step(v=v_t,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ti_s=i_s_t,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ti_f=i_f_t,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ti_e=i_e_t,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tx=x_t,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\ta=a_t,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tl=L_mat,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tj_slow=J_slow,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tj_fast=J_fast,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tw=W_mat,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tu_slow=U_slow,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tu_fast=U_fast)\n",
    "\t\n",
    "\tV_MEMBRANE_POTENTIALS[:,idx+1] = v_new\n",
    "\tI_S_SLOW_CURRENTS[:,idx + 1] = i_s_new\n",
    "\tI_F_FAST_CURRENTS[:,idx + 1] = i_f_new\n",
    "\tX_RESOURCE_STATE_VAR[:,idx + 1] = x_new\n",
    "\tA_SPIKING_STATE_VAR[:,idx + 1] = a_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_MEMBRANE_POTENTIALS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(TIMELINE.cpu(),V_MEMBRANE_POTENTIALS[0].cpu())\n",
    "plt.xlim(0,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(TIMELINE.cpu(),I_F_FAST_CURRENTS[1].cpu())\n",
    "plt.xlim(0,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We definitely see the rapid inhibitory currents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(I_S_SLOW_CURRENTS[1].cpu())\n",
    "plt.xlim(0,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedlearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
